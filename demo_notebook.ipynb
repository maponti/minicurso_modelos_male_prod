{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Material do minicurso \"Introdução ao Desenvolvimento de Modelos de Aprendizado de Máquina Produtivos\"\n",
        "\n",
        "#### Moacir A. Ponti - 2024\n",
        "---\n",
        "# Demo notebook\n",
        "\n",
        "1. Compactar a pasta src + arquivo `model_params.json` em um arquivo `src.zip`\n",
        "2. Subir o arquivo no colab\n",
        "\n"
      ],
      "metadata": {
        "id": "FCfrSiGOiow4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "msoHDRqSkOWH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r \"/content/src\"\n",
        "!rm \"/content/model_params.json\"\n",
        "!unzip \"/content/src.zip\"\n",
        "!rm \"/content/src.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_UhTRLPUZhv",
        "outputId": "5f0f9f2e-45f0-433b-befd-3eb3999ea0cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/src': No such file or directory\n",
            "rm: cannot remove '/content/model_params.json': No such file or directory\n",
            "Archive:  /content/src.zip\n",
            "  inflating: model_params.json       \n",
            "   creating: src/\n",
            "   creating: src/data/\n",
            "  inflating: src/data/etl.py         \n",
            "  inflating: src/data/run_etl.py     \n",
            "  inflating: src/headers_.py         \n",
            "  inflating: src/inference.py        \n",
            "   creating: src/model/\n",
            "  inflating: src/model/model.py      \n",
            "  inflating: src/model/run_training.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.data.etl import ETL\n",
        "from src.model.model import Model\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9YU_XpqKD0c",
        "outputId": "4908e2f7-0a8e-4f74-96ef-5322048b946b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and defining functions for demo at Colab\n",
        "\n",
        "- those functions are in the files `run_etl.py` and `run_training.py`\n",
        "- in real productive scenarios we would like to run those scripts, and never a notebook\n",
        "- however for this demo I am copying some of the functions to show and explain them"
      ],
      "metadata": {
        "id": "q-T26t56W1kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(dataset_artifact_file, model_version, model_params_file):\n",
        "    \"\"\" Trains and Evaluate model\n",
        "    Args:\n",
        "        dataset_artifact_file: str - the path to the dataset artifact file\n",
        "        model_version: str - the model version\n",
        "        model_params_file: str - the path to the model parameters file\n",
        "    Returns:\n",
        "        tuple: model, dictionary with evaluation\n",
        "    \"\"\"\n",
        "    # create an instance of the Model class\n",
        "    model = Model(model_version)\n",
        "\n",
        "    # loads dataset artifact from the pickle file\n",
        "    dataset_artifact = ETL.deserialize_dataset(dataset_artifact_file)\n",
        "    if not dataset_artifact:\n",
        "        print('Could not load dataset artifact')\n",
        "        return\n",
        "\n",
        "    # validate dataset\n",
        "    if not ETL.validate_dataset(dataset_artifact):\n",
        "        print('Dataset is not valid')\n",
        "        return\n",
        "\n",
        "    # loads JSON into a dictionary\n",
        "    with open(model_params_file, 'r') as file:\n",
        "        model_params = json.load(file)\n",
        "\n",
        "    ignore_columns = [col for col in dataset_artifact['dataset'].columns if col not in dataset_artifact['features']]\n",
        "\n",
        "    X, X_test, y, y_test = model.create_train_test_split(dataset_artifact, test_size=0.2, split_by='random',\n",
        "                                                         time_column=None, target_column='target',\n",
        "                                                         ignore_columns=ignore_columns)\n",
        "\n",
        "    model.train(X, y, model_params)\n",
        "    dict_eval = model.evaluate_model(X_test, y_test, dataset_version=dataset_artifact['version'], verbose=True)\n",
        "\n",
        "    return model, dict_eval"
      ],
      "metadata": {
        "id": "09m-DZA5KCrJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# content of inference.py\n",
        "import numpy as np\n",
        "\n",
        "def load_model(model_file):\n",
        "    \"\"\"\n",
        "    Loads the model from a pickle file\n",
        "    Args:\n",
        "        model_file: str - the path to the pickle file\n",
        "    Returns:\n",
        "        model: the model\n",
        "    \"\"\"\n",
        "    model = Model.load(model_file)\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, data):\n",
        "    \"\"\"\n",
        "    Predicts the target\n",
        "    Args:\n",
        "        model: the model\n",
        "        data: np.array - the data to predict\n",
        "    Returns:\n",
        "        np.array: the predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # check if json contain all the features as defined in model.features, and if not imput with NULL\n",
        "    # predict\n",
        "    scores, missing_features = model.predict(data)\n",
        "\n",
        "    response = {'score': round(float(scores[1]), 4),\n",
        "                'no_missing_features': missing_features}\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "08N1FkDlY-yt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: using a public dataset\n",
        "\n",
        "California housing:\n",
        "- As target we will predict the median house values above 90K USD"
      ],
      "metadata": {
        "id": "vXItlPsAZHlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) ETL"
      ],
      "metadata": {
        "id": "bXi_jc-3Mwif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = '/content/sample_data/california_housing_train.csv'\n",
        "\n",
        "### this content is also at the run_etl.py file\n",
        "\n",
        "# create an instance of the ETL class\n",
        "etl = ETL()\n",
        "# load the dataset from the csv file\n",
        "dataset = etl.load_dataset_from_csv(csv_file)\n",
        "\n",
        "# select which columns will be used as features of the model\n",
        "etl.select_columns(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
        "       'total_bedrooms', 'population', 'households', 'median_income'])"
      ],
      "metadata": {
        "id": "romOguQTKjnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a644880-f25f-47dd-c053-da0fff3c1b95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
              "       'total_bedrooms', 'population', 'households', 'median_income'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# as part of the ETL process we define also a \"target\" column\n",
        "\n",
        "# in this case we will transform the median_house_value values above 90K as target 1, while the remaining will be target 0\n",
        "dataset.rename({'median_house_value':'target'}, axis=1, inplace=True)\n",
        "dataset['target'] = (dataset['target'] > 90000).astype(int)"
      ],
      "metadata": {
        "id": "GMTVcNAfuTOK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build dictionary containing dataset information\n",
        "dataset_artifact = {'name': 'california_housing',\n",
        "                    'version': '1.0.0',\n",
        "                    'features': etl.selected_features,\n",
        "                    'dataset': dataset}\n",
        "\n",
        "# serialize it (creates an artifact that can be stored and versioned, later retrieved)\n",
        "etl.serialize_dataset(dataset_artifact)"
      ],
      "metadata": {
        "id": "-adg6VvOKzlj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_artifact['features']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdoU-QQpLroy",
        "outputId": "3631d5f7-f666-4397-ddf4-9028cf727aef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
              "       'total_bedrooms', 'population', 'households', 'median_income'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "- requires a dataset artifact\n",
        "- requires a `json` file with the model parameters"
      ],
      "metadata": {
        "id": "M8RisGVeJ1vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_artifact_file = 'dataset_california_housing_1.0.0.pkl'\n",
        "model_params_file = 'model_params.json'\n",
        "\n",
        "if not dataset_artifact_file.endswith('.pkl') or not model_params_file.endswith('.json'):\n",
        "    print('Please provide a valid pickle file and json file')\n",
        "\n",
        "# check params json file exists\n",
        "if not os.path.exists(model_params_file):\n",
        "    print(f'Model Params file: {model_params_file} not found')"
      ],
      "metadata": {
        "id": "u44AoxFaL9uI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_version = '1.0.0'\n",
        "\n",
        "# run training and evaluation\n",
        "model, dict_eval = train_and_evaluate(dataset_artifact_file, model_version, model_params_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgv0A2XYKveZ",
        "outputId": "6ae6929b-4978-4c81-a0fc-6a47c3c6f100"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Model (v=1.0.0) evaluation:\n",
            "Test set version = 1.0.0, size = 3400 rows x 8 feats\n",
            "\tPRAUC: 0.9524\n",
            "\tROCAUC: 0.8426\n",
            "\tF1: 0.9642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# serializes model in a file\n",
        "model.serialize()"
      ],
      "metadata": {
        "id": "S73TT58mNHjV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates an instance example to be used later in the inference test\n",
        "model.instance_example_to_json()"
      ],
      "metadata": {
        "id": "6D8Kg7yHVVkC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defines the model file and the data file to be predicted\n",
        "model_file = 'model_1.0.0.pkl'\n",
        "data_file = 'instance.json'\n",
        "\n",
        "# the code below is in inference.py script\n",
        "\n",
        "# loads model from pickle\n",
        "model = load_model(model_file)\n",
        "# loads data from json\n",
        "with open(data_file, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "print(f'Inference with model {model.model_name}, version {model.version}...')\n",
        "# predict\n",
        "response = predict(model, data)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Z5xmvvYNkz",
        "outputId": "ef8a1131-9e3d-4dfe-c4ce-027fd101e679"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference with model lightgbm, version 1.0.0...\n",
            "{'score': 1.0, 'no_missing_features': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/src/inference.py model_1.0.0.pkl instance.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDnbIy4tNXGr",
        "outputId": "2b8a4b07-9110-4983-d16b-8b08d2e99db3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    }
  ]
}